{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProbabilisticProgramming.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/euxoa/ompeluseura/blob/master/ProbabilisticProgramming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neYKc6Nc9Ij9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUPcI2BI9LSv",
        "colab_type": "text"
      },
      "source": [
        "# Probabilistic programming\n",
        "\n",
        "(This is not about generic probabilistic programming, see https://arxiv.org/abs/1809.10756, but about how and why to do bayesian models programmatically.)\n",
        "\n",
        "## Generative models and their uses\n",
        "\n",
        "A generative model describes \n",
        "how your data possibly arise, \n",
        "conditioned on the parameters of the model, \n",
        "including parameterized uncertainty that is,\n",
        "things that you cannot or do not want to model.\n",
        "\n",
        "In PP, you set up a generative model for your data, and let the PP system infer what the parameters might be. Because of uncertainty in your model, you will not get a definite answer, but typically a set of parameter scenarios or samples. These and their variations may then be useful:\n",
        "* Inference: “The weather station has warmed 0.7±0.2 °C during the last decade.”\n",
        "* Uncertainty: “±0.2°, that is a 95% confidence interval.”\n",
        "* Hypothesis testing: “The observed difference between groups is within random fluctuation, so no support for the idea they are different.”\n",
        "* Prediction: “The model indicates that GDP is likely to drop 0.2% during the next quartal.”\n",
        "* Prediction: “Based on other users and their preferences, we recommend you these books..”\n",
        "* Decisions: “Our expected loss (risk, whatever) is minimized by doing A instead of B.” \n",
        "\n",
        "Note that often you can estimate a generative model by finding the maximum of posterior (MAP), which is almost the same as maximum likelihood (ML). This is fast, but does not always work, is sometimes misleading, and omits the uncertainty.\n",
        "\n",
        "Parametric approximative techniques are available for uncertainty of MAP, and for the whole posterior. We don't deal with these techniques here.\n",
        "\n",
        "## Examples of generative models:\n",
        "* Ordinary regression:\n",
        "y arises as a function of covariates, plus noise\n",
        "(b and sigma arises from some fuzzy distribution left unspecified)\n",
        "y = x ‘ b + N(0, sigma)\n",
        "* Autoregressive time series model:\n",
        "y arises as a function of previous y, plus noise\n",
        "(a again, comes from something random, left unspecified but within [0, 1])\n",
        "y(t) = a y(t-1) + (1-a) N(0, sigma)\n",
        "* Latent random walk for time series:\n",
        "y(t) = mu(t) + N(0, sigma_obs)\n",
        "mu(t) = mu(t-1) + N(0, sigma_rw)\n",
        "Note that all mu(t) are parameters. More parameters than observations! But this is ok.\n",
        "* A biased coin:\n",
        "y_i ~ Binomial(theta);\n",
        "* Many coins each with different unknown bias, and a distribution for the biases:\n",
        "                y_ki ~ Binomial(theta_k);\n",
        "                theta_k ~ Beta(a1, a2);\n",
        "\n",
        "\n",
        "In general, you define how data depends on parameters, p(y | theta), called likelihood, and how parameters are distributed if you have no data at all, p(theta), called prior. PP gives you samples from p(theta | y), or the posterior distribution of parameters given the data.\n",
        "\n",
        "\n",
        "## Why bayes?\n",
        "* Need good, detailed estimates of uncertainty.\n",
        "* You don’t have a clear unit of independent data, but a hierarchy, lateral dependencies in time or space, or qualitatively different sets of observations. \n",
        "* You want to combine old model (or strong prior) with new data (model update).\n",
        "* You have so few data that uncertainty dominates.\n",
        "\n",
        "\n",
        "## Why probabilistic programming?\n",
        "* Most practical problems are somehow non-standard. Getting the details right is critical for some reason. Standard packages then do not apply, you need a custom model.\n",
        "* Your setup, data or model have non-trivial structure, no hope with standard packages at all. \n",
        "* You don’t have three months to derive the gradients and write a sampler.\n",
        "\n",
        "\n",
        "## Why not probabilistic programming?\n",
        "* You data is too big, computation would take forever.\n",
        "* No time, just give me some estimates!\n",
        "\n",
        "\n",
        "## Levels:\n",
        "* Use a specialized package: in R, brms, rstan_arm; Facebook’s Prophet, …; you don’t even know you are using PP.\n",
        "* Write raw likelihood: Stan, PyMC, Tensorflow Probability, …\n",
        "* There are also approximate inference techniques: variational, etc., but these are for advanced users only.\n",
        "* Research on inference techniques, deep probabilistic models, etc.\n",
        "\n",
        "\n",
        "## Given data, and model, how do you get the parameters?\n",
        "* In principle easy: p(theta | y) \\propto p(y | theta) p(theta)\n",
        "* In practice you cannot typically solve the distribution of theta!\n",
        "   * Finding normalization would require very non-trivial integration.\n",
        "* High dimensionality of theta, no grids etc.\n",
        "* But there are ways to get samples from the posterior.\n",
        "* Sample means a set of possible (parameter) worlds behind data, or\n",
        "scenarios of parameters. \n",
        "   * “It seems that the tulips are either yellow and small, or big and a bit darker, but we don’t know which one.”\n",
        "\n",
        "\n",
        "We have platforms that do gradients and find an optimum for you (Tensorflow etc.)\n",
        "We have platforms that do gradients and sampling, and find the posterior distribution for you (Stan etc.)\n",
        "\n",
        "\n",
        "## Probabilistic programming workflow:\n",
        "   * Typically you write the data-generating process. \n",
        "   * The platform computes the likelihood (if it's not already obvious).\n",
        "   * The platform computes gradients of your code.\n",
        "   * The platform samples (nontrivial: tuning and utilizing gradients)\n",
        "   * You get a sample from the posterior.\n",
        "   * You make quality checks.\n",
        "\n",
        "\n",
        "## Sampling itself is nontrivial\n",
        "   * For good sampling, gradients are required.\n",
        "   * Sampler needs tuning, which with PP platforms is automatic.\n",
        "   * Still, quality checks are needed, but these are now partly automatic."
      ]
    }
  ]
}