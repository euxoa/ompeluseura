{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProbabilisticProgramming.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/euxoa/ompeluseura/blob/master/ProbabilisticProgramming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUPcI2BI9LSv",
        "colab_type": "text"
      },
      "source": [
        "# Probabilistic programming\n",
        "\n",
        "(This is about how and why to do bayesian models programmatically. More generally about probabilistic programming, see Wikipedia or https://arxiv.org/abs/1809.10756.\n",
        "\n",
        "## Generative models and their uses\n",
        "\n",
        "A generative model describes \n",
        "* how your data possibly arise,\n",
        "* conditioned on the _parameters_ of the model, \n",
        "* including parameterized uncertainty, that is, things that you cannot or do not want to model.\n",
        "\n",
        "In probabilistic  programming (PP), you set up a generative model for your data, and let the PP system infer what the parameters might be. Because of uncertainty in your model, you will not get a definite answer, but typically a set of _parameter scenarios or samples_. These and their variation may then be useful:\n",
        "* Inference: _“The weather station has warmed 0.7±0.2 °C during the last decade.”_\n",
        "* Uncertainty: _“±0.2° (95% confidence interval)”_\n",
        "* Hypothesis testing: _“The observed difference between groups is within expected random fluctuation, so no support for the idea they are different.”_\n",
        "* Prediction: _“The model indicates GDP drop of 0.2% during the next quartal.”_\n",
        "* Prediction: _“Based on other users and their preferences, we recommend you these books...”_\n",
        "* Decisions: _“Our expected loss (or risk, or a combination) is minimized by doing A instead of B.”_\n",
        "\n",
        "Note that often you can estimate a generative model by finding the maximally probable parameters (_maximum a posteriori_, MAP), which is almost the same as maximum likelihood (ML). This is fast, but does not work for all models, is sometimes misleading, and omits the uncertainty.\n",
        "\n",
        "Parametric approximative techniques are available for uncertainty of MAP, and for the whole posterior. We don't deal with these techniques here.\n",
        "\n",
        "## Examples of generative models:\n",
        "* Ordinary regression:\n",
        "$y$ arises as a function of covariates, plus noise\n",
        "($b$ and $\\sigma$ arise from some fuzzy distribution left unspecified)\n",
        "$$y = x ^T b + N(0, \\sigma)$$\n",
        "* Random-walk time-series model:\n",
        "$y$ arises as a function of previous $y$, plus noise\n",
        "($a$ again comes from something random, left unspecified but within $[0, 1]$)\n",
        "$$y(t) = a y(t-1) + (1-a) N(0, \\sigma)$$\n",
        "* Latent random walk for time series:\n",
        "$$y(t) = mu(t) + N(0, \\sigma_\\textit{obs})$$\n",
        "$$mu(t) = mu(t-1) + N(0, \\sigma_\\textit{rw})$$\n",
        "Note that all $mu(t)$ are parameters. More parameters than observations! But this is ok.\n",
        "* A biased coin:\n",
        "$$y_i \\sim \\textrm{Bernoulli}(\\theta)$$\n",
        "* Many coins, each with different unknown bias, and a distribution for the biases:\n",
        "$$y_ki \\sim \\textrm{Bernoulli}(\\theta_k)$$\n",
        "$$\\theta_k \\sim \\textrm{Beta}(a_1, a_2)$$\n",
        "\n",
        "\n",
        "In general, you define how data $y$ depends on parameters, $p(y | \\theta)$, called likelihood, and how parameters are distributed if you have no data at all, $p(\\theta)$, called prior. PP gives you samples from $p(\\theta | y)$, or the posterior distribution of parameters given the data.\n",
        "\n",
        "\n",
        "## Why bayes?\n",
        "* Need good, detailed estimates of uncertainty.\n",
        "* You don’t have a clear unit of independent data, but a hierarchy, lateral dependencies in time or space, or qualitatively different sets of observations. \n",
        "* You want to combine old model (or strong prior) with new data (model update).\n",
        "* You have so few data that uncertainty dominates.\n",
        "\n",
        "\n",
        "## Why probabilistic programming?\n",
        "* Most practical problems are somehow non-standard. Getting the details right is critical for some reason. Standard packages then do not apply, you need a custom model.\n",
        "* Your setup, data or model have non-trivial structure, no hope with standard packages at all. \n",
        "* You don’t have three months to derive the gradients and write a sampler.\n",
        "\n",
        "\n",
        "## Why not probabilistic programming?\n",
        "* You data is too big, computation would take forever.\n",
        "* No time, just give me some estimates, any estimates better than guessing!\n",
        "\n",
        "\n",
        "## Levels:\n",
        "* Use a specialized package: in R, brms, rstan_arm; Facebook’s Prophet, …; you don’t even know you are using PP.\n",
        "* Write raw likelihood: Stan, PyMC, Tensorflow Probability, …\n",
        "* There are also approximate inference techniques: variational, etc., but these are maybe for advanced users only.\n",
        "* Research on inference techniques, deep probabilistic models, etc.\n",
        "\n",
        "\n",
        "## Given data, and model, how do you get the parameters?\n",
        "* In principle easy: $$p(\\theta | y) \\propto p(y | \\theta) p(\\theta)$$\n",
        "* In practice you cannot typically solve the distribution of $\\theta$.\n",
        "   * Finding normalization would require very non-trivial integration.\n",
        "* High dimensionality of $\\theta$, so no grids of parameters.\n",
        "* But there are ways to get samples from the posterior.\n",
        "* Sample means a set of possible (parameter) worlds behind data, or\n",
        "scenarios of parameters: \n",
        "   * “It seems that the tulips are either yellow and small, or big and a bit darker, but we don’t know which one.”\n",
        "* For continuous distributions, best samplers are based on gradients or even higher derivatives of the posterior distribution.\n",
        "\n",
        "Some platforms do gradients and find an optimum for you (Tensorflow etc.).\n",
        "Other platforms do gradients and sampling, and find the posterior distribution for you (Stan etc.)\n",
        "\n",
        "\n",
        "## Probabilistic programming workflow:\n",
        "   * Typically you write the data-generating process, as a code.\n",
        "   * The platform computes the likelihood (if it's not already obvious).\n",
        "   * The platform computes gradients of your code.\n",
        "   * The platform samples (nontrivial: tuning and utilizing gradients).\n",
        "   * You get a sample from the posterior.\n",
        "   * You make quality checks.\n",
        "\n",
        "\n",
        "## Sampling itself is nontrivial\n",
        "   * For good sampling, gradients are required.\n",
        "   * Sampler needs tuning, which with PP platforms is automatic.\n",
        "   * Still, quality checks are needed, but these are now partly automatic."
      ]
    }
  ]
}