{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Kaisaniemi-Snow-Python.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFhoDVXztWxR",
        "colab_type": "text"
      },
      "source": [
        "# Depth of snow cover in Kaisaniemi Helsinki"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP-hlWb_tWxU",
        "colab_type": "text"
      },
      "source": [
        "This notebook investigates the depth of snow in Kaisaniemi Helsinki over last 60 years. Especially we look in to the probability of snow depth being more than 0 cm on a single day each year.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHJDulrttWxU",
        "colab_type": "text"
      },
      "source": [
        "The data is originally from FMI: https://ilmatieteenlaitos.fi/havaintojen-lataus#!/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "632LXgCdtem7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install packages not available by default in Colab\n",
        "!pip install arviz \n",
        "import arviz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UUms6UlztWxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import pystan\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.special as ss\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_cfzDCJai5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show warnings only once\n",
        "import warnings\n",
        "warnings.filterwarnings(action='once')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0xwiJJwtWxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and cleanup data\n",
        "# ovewrite original column names\n",
        "# parse dates from several columns\n",
        "# clean up negative snow depths\n",
        "# About snow measurement in finnish: \n",
        "# Tilastoitava lumensyvyys mitataan aamulla klo 8 paikallista aikaa (kesäaikaan klo 9). Arvo -1 = ei lunta. Arvo 0 = havaintoasemalla ei ole lunta, mutta sen ympäristössä aukealla on.\n",
        "df = (pd.read_csv(\"https://raw.githubusercontent.com/dins/snow-depth/master/kaisaniemi.csv\", \n",
        "                 names=['year', 'month', 'day', 'clock', 'tzone', 'snow', 'temp'],\n",
        "                 header=0)\n",
        "                .assign(date = lambda d: pd.to_datetime(d[['year', 'month', 'day']]),\n",
        "                        snow = lambda d: d['snow'].clip(0),\n",
        "                        is_snow = lambda d: d['snow'] > 0)\n",
        "                  [['date', 'snow', 'is_snow', 'temp']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9b7r9PTtWxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHVfmKwrtWxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look into a specific day of the year\n",
        "# Remove years without snow depth measurement\n",
        "christmas = df.loc[lambda d: ~d['snow'].isnull() & (d['date'].dt.day == 24) & (d['date'].dt.month == 12)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjSX2whVtWxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "christmas.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-TcPNWbtWxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.scatterplot(x=\"date\", y=\"snow\", hue=\"is_snow\", data=christmas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itbxEou_tWxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stan_data = christmas.assign(decade=lambda d: (d['date'].dt.year - 2000) / 10,\n",
        "                             is_snow=lambda d: d['is_snow'].astype(int))[['decade', 'is_snow']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpDTlORsH-y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wap5uSk2SiG",
        "colab_type": "text"
      },
      "source": [
        "# First simple model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZRHPYriIkbE",
        "colab_type": "text"
      },
      "source": [
        "Logit is a function that creates a map of probability values from [0,1] to (-infinity, infinity): https://en.wikipedia.org/wiki/Logit\n",
        "\n",
        "We use it here to transform probability values to real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oJUA_EDtWxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_model_code = '''\n",
        "data {\n",
        "   int N;\n",
        "   int<lower=0, upper=1> is_snow[N];\n",
        "}\n",
        "parameters {\n",
        "   real b;\n",
        "}\n",
        "model {\n",
        "  for (i in 1:N) {\n",
        "    is_snow[i] ~ bernoulli_logit(b);\n",
        "  }\n",
        "}\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syFNZiRDtWxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = pystan.StanModel(model_code=first_model_code)\n",
        "fit = model.sampling(data={'N': len(stan_data), **stan_data.to_dict(orient='list')}, iter=1000, chains=4)\n",
        "fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVJSse-GY6l8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arviz.plot_trace(fit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dxP7RPO8Fx",
        "colab_type": "text"
      },
      "source": [
        "The posterior distribution summarizes what you know after the data has been observed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOP4c1lhtWxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_posterior = fit.extract('b')['b'] # Extract posterior draws for the parameter b. These sample values represent our posterior distribution.\n",
        "sns.distplot(b_posterior, fit=norm) # Looks like normally distributed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu3Z5ufEe8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Similar plot with Arviz\n",
        "arviz.plot_posterior(fit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL0WH8hpg4OC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "arviz.plot_density(fit);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLvnhsv2NdyH",
        "colab_type": "text"
      },
      "source": [
        "Let's undo the logit transformation i.e. convert b values from real scale to probability scale. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "kLpi18XjtWxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_posterior = ss.expit(b_posterior) # inverse logit\n",
        "arviz.plot_posterior(p_posterior)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tULplD0XsyN",
        "colab_type": "text"
      },
      "source": [
        "# A bit more complex model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c5QFvdIYHUk",
        "colab_type": "text"
      },
      "source": [
        "Here we try to estimate the trend over time. We fit logistic regression to the is_snow variable.\n",
        "\n",
        "Also we do the inverse logit transform in Stan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PboY_HYtWxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_model_code = '''\n",
        "data {\n",
        "  int N;\n",
        "  int<lower=0, upper=1> is_snow[N];\n",
        "  real decade[N]; \n",
        "}\n",
        "parameters {\n",
        "  real b;\n",
        "  real k; \n",
        "}\n",
        "model {\n",
        "  for (i in 1:N) {\n",
        "    is_snow[i] ~ bernoulli_logit(k * decade[i] + b);\n",
        "  }\n",
        "}\n",
        "generated quantities {\n",
        "  real prob[N];\n",
        "  for (i in 1:N) {\n",
        "    prob[i] = inv_logit(k* decade[i] + b);\n",
        "  }\n",
        "}\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IERMpE2dtWxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = pystan.StanModel(model_code=second_model_code)\n",
        "fit2 = model2.sampling(data={'N': len(stan_data), **stan_data.to_dict(orient='list')}, iter=1000, chains=4)\n",
        "fit2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkVjYzPG5J5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with arviz.rc_context(rc={'plot.max_subplots': 10}): # Hangs if we try to plot all the 62 variables\n",
        "  arviz.plot_trace(fit2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7shf7ausdaJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arviz.plot_trace(fit2, var_names=['b', 'k'])\n",
        "arviz.plot_posterior(fit2, var_names=['b', 'k'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSU70NSWtWxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Has snow cover decreased significantly?\n",
        "params = fit2.extract(['prob[1]', 'prob[60]'])\n",
        "diff_samples = params['prob[1]'] - params['prob[60]']\n",
        "# probability of decrease\n",
        "np.mean(diff_samples > 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UNBhwX6tWxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now estimate snow propability for 2019\n",
        "decade_2019 = (2019 - 2000) / 10\n",
        "post_draws = fit2.extract(['b', 'k'])\n",
        "predictions = ss.expit(post_draws['b'] + post_draws['k'] * decade_2019)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt44p5HFtWx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(predictions).round(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8F8FBvFtWx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1tBT-KjtWx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arviz.plot_posterior(predictions) # ONKO JÄRKEVÄ?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcdeJoehwUZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}